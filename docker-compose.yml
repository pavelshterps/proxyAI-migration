version: '3.8'

services:
  db:
    image: postgres:15
    restart: always
    environment:
      POSTGRES_DB:   ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - proxyai_net

  redis:
    image: redis:7-alpine
    restart: unless-stopped
    volumes:
      - redis-data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      retries: 3
    networks:
      - proxyai_net

  # ==== API (FastAPI) ====
  api:
    build:
      context: .
      dockerfile: Dockerfile
    env_file: .env
    # вместо celery, запускаем Uvicorn
    command: >
      uvicorn main:app
      --host 0.0.0.0
      --port 8000
      --workers ${API_WORKERS}
    environment:
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      - USE_EXTERNAL_TRANSCRIBE
    volumes:
      - ./:/app:delegated
      - upload-data:/data/uploads
      - results-data:/data/results
      - diarizer-cache:/data/diarizer_cache
      # монтируем модели Whisper из /data/whisper-models
      - /data/whisper-models:/hf_cache:ro
    ports:
      - "8000:8000"
    depends_on:
      - db
      - redis
    restart: unless-stopped
    networks:
      - proxyai_net

  # ==== CPU-воркер ====
  cpu-worker:
    build:
      context: .
      dockerfile: Dockerfile
    env_file: .env
    environment:
      - WHISPER_DEVICE=cpu
    volumes:
      - ./:/app:delegated
      - upload-data:/data/uploads
      - results-data:/data/results
      - diarizer-cache:/data/diarizer_cache:ro
      - /data/whisper-models:/hf_cache:ro
    command: >
      celery -A config.celery:app
      worker
      --queues preprocess_cpu
      --concurrency ${CPU_CONCURRENCY}
      --loglevel info
    depends_on:
      - db
      - redis
    restart: unless-stopped
    networks:
      - proxyai_net

  # ==== GPU-воркер ====
  gpu-worker:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    env_file: .env
    environment:
      - WHISPER_DEVICE=cuda
      - NVIDIA_VISIBLE_DEVICES=all
    runtime: nvidia
    volumes:
      - ./:/app:delegated
      - upload-data:/data/uploads
      - results-data:/data/results
      - diarizer-cache:/data/diarizer_cache
      - /data/whisper-models:/hf_cache:ro
    command: >
      celery -A config.celery:app
      worker
      --queues preprocess_gpu
      --loglevel info
      --pool solo
    depends_on:
      - db
      - redis
    restart: unless-stopped
    networks:
      - proxyai_net

  # ==== Celery Beat ====
  beat:
    build:
      context: .
      dockerfile: Dockerfile
    env_file: .env
    command: >
      celery -A config.celery:app
      beat
      --loglevel info
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - proxyai_net

  # ==== Flower UI ====
  flower:
    image: mher/flower:0.9.7
    env_file: .env
    ports:
      - "5555:5555"
    command:
      - flower
      - "--broker=${CELERY_BROKER_URL}"
      - "--basic_auth=${FLOWER_USER}:${FLOWER_PASS}"
      - "--port=5555"
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - proxyai_net

  # ==== Nginx (reverse proxy + SSL) ====
  nginx:
    image: nginx:stable-alpine
    container_name: proxyai_nginx
    depends_on:
      - api
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - /etc/letsencrypt/live/tonage.io/fullchain.pem:/etc/nginx/certs/fullchain.pem:ro
      - /etc/letsencrypt/live/tonage.io/privkey.pem:/etc/nginx/certs/privkey.pem:ro
    restart: unless-stopped
    networks:
      - proxyai_net

volumes:
  postgres_data:
  redis-data:
  upload-data:
  results-data:
  diarizer-cache:
  hf-cache:  # оставляем (для локального кэша), но теперь не монтируется напрямую

networks:
  proxyai_net:
    driver: bridge